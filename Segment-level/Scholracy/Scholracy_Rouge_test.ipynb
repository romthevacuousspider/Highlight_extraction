{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"text-align: center;\">Scholracy ROUGE score</h1>"
      ],
      "metadata": {
        "id": "wbnparrJ6-h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for finding the baseline of [Scholracy](https://www.scholarcy.com/) regarding the ROUGE score."
      ],
      "metadata": {
        "id": "HjS5ylXg8Lzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "RYIH6J5aJZC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to drive\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "colab_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', colab_path)\n",
        "sys.path.insert(0,colab_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcyXKh8kJjYc",
        "outputId": "2a617c23-09dd-4e81-87dd-cb40b24940b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhczWVexJcQX",
        "outputId": "ec63e168-46e1-4f49-fec0-98b99af00ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=bb855c94ac6e8ed2b3b66bd85ac422beb3061a8f2adaad33f4dbc0a5c1cb449f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE score"
      ],
      "metadata": {
        "id": "2qXWKuTc7S4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_scores(hypotheses, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    for hyp, ref in zip(hypotheses, references):\n",
        "        score = scorer.score(hyp, ref)\n",
        "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "    avg_scores = {metric: sum(values) / len(values) for metric, values in scores.items()}\n",
        "    return avg_scores\n"
      ],
      "metadata": {
        "id": "9FXymQd0JdKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Dataset holding Scholracy results on test set (same as T5 notebook)\n",
        "df = pd.read_csv('/content/drive/MyDrive/Final Project/Field Project/Highlighted Papers/Scholary_sumarize - Copy.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "U5M8Uu9IJz6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(\"\\n\", inplace=True) #as we have paragraphs without anything worth to be highlighted"
      ],
      "metadata": {
        "id": "wGpg0jYrKPpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses = df['result'].tolist()\n",
        "references = df['Chunk'].tolist()\n"
      ],
      "metadata": {
        "id": "cTVnhDPXKQFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = calculate_rouge_scores(hypotheses, references)\n",
        "\n",
        "print(\"ROUGE-1: {:.4f}\".format(rouge_scores['rouge1']))\n",
        "print(\"ROUGE-2: {:.4f}\".format(rouge_scores['rouge2']))\n",
        "print(\"ROUGE-L: {:.4f}\".format(rouge_scores['rougeL']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHHZamGdKsI2",
        "outputId": "dee11895-cd2f-4b4d-ef2c-8a4807740b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1: 0.1928\n",
            "ROUGE-2: 0.1591\n",
            "ROUGE-L: 0.1776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "O3kHLG_I7-Sa"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}